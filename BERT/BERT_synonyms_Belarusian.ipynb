{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiELzZbP-TXV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X4UFFO8gn3cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n",
        "import random"
      ],
      "metadata": {
        "id": "E5zjxlbw-ZWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "eTqP6glp-bck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "IHA4UxYA-dzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дообучение модели"
      ],
      "metadata": {
        "id": "NFEabaWXB-T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_mask(text, target_word, tokenizer):\n",
        "    target_tokens = tokenizer.tokenize(target_word)\n",
        "    num_target_tokens = len(target_tokens)\n",
        "\n",
        "    if \"[MASK]\" in text:\n",
        "        return text.replace(\"[MASK]\", \" \".join([\"[MASK]\"] * num_target_tokens), 1)\n",
        "    else:\n",
        "        raise ValueError(f\"Ошибка: в тексте нет [MASK] → {text}\")"
      ],
      "metadata": {
        "id": "UEV2fauh-gbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SynonymDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"context\"]\n",
        "        target_word = item[\"word\"]\n",
        "\n",
        "        if \"[MASK]\" not in text:\n",
        "            raise ValueError(f\"Ошибка: в тексте нет [MASK] → {text}\")\n",
        "\n",
        "        text = expand_mask(text, target_word, self.tokenizer)\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = torch.full_like(encoding[\"input_ids\"], -100)\n",
        "\n",
        "        mask_idx = (encoding[\"input_ids\"] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "        target_ids = self.tokenizer.encode(target_word, add_special_tokens=False)\n",
        "\n",
        "        if len(mask_idx) != len(target_ids):\n",
        "            raise ValueError(f\"Ошибка маскировки: {len(mask_idx)} масок, но '{target_word}' = {len(target_ids)} токенов\")\n",
        "\n",
        "        labels[0, mask_idx] = torch.tensor(target_ids)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": labels.squeeze()\n",
        "        }"
      ],
      "metadata": {
        "id": "UxtkINQacANg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bert(json_file, model_name=\"KoichiYasuoka/roberta-small-belarusian\", output_dir=\"./bert-synonyms\"):\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    dataset = SynonymDataset(data, tokenizer)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=16,\n",
        "        learning_rate=5e-5,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "\n",
        "train_bert(\"train_data.json\")"
      ],
      "metadata": {
        "id": "IbCClpqvveWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NwYeQJSxWTAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='be', processors='tokenize,pos,lemma')"
      ],
      "metadata": {
        "id": "4XPwXYUtWS79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(word):\n",
        "    doc = nlp(word)\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            return token.words[0].lemma\n",
        "    return word"
      ],
      "metadata": {
        "id": "uKAnBlSQzQTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка словаря"
      ],
      "metadata": {
        "id": "y6nJiJuNCFud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def load_synonyms_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    synonym_dict = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        word = str(row[0]).strip()\n",
        "        lemma = lemmatize(word)\n",
        "        synonyms = str(row[1]).split(',')\n",
        "        synonym_dict[lemma].extend(s.strip() for s in synonyms if s.strip())\n",
        "    return synonym_dict"
      ],
      "metadata": {
        "id": "nCsVaE-Q1B_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_dict = load_synonyms_from_excel(\"synonyms_table_result.xlsx\")"
      ],
      "metadata": {
        "id": "jR4AJZA8hFrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ранжирование предсказаний модели"
      ],
      "metadata": {
        "id": "PeNCGD78CIGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_masked_index(tokens):\n",
        "    try:\n",
        "        return tokens.index('[MASK]')\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Токен [MASK] не найден в списке токенов.\")\n",
        "\n",
        "\n",
        "# --- Получение предсказаний и добавление синонимов из словаря ---\n",
        "def get_masked_predictions(sentence, original_word, synonym_dict=None, top_k=100):\n",
        "    encoding = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoding[0])\n",
        "\n",
        "    masked_index = find_masked_index(tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=encoding)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    predicted_ids = torch.topk(predictions[0, masked_index], top_k).indices.numpy()\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids)\n",
        "\n",
        "    filtered_words = [word for word in predicted_tokens if word.isalpha() and len(word) >= 3]\n",
        "\n",
        "    if synonym_dict:\n",
        "        lemma = lemmatize(original_word)\n",
        "        if lemma in synonym_dict:\n",
        "            extra_synonyms = synonym_dict[lemma]\n",
        "            filtered_words = list(set(filtered_words + extra_synonyms))\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "\n",
        "# --- Ранжирование по сходству предложений ---\n",
        "def rank_by_similarity(original_sentence, predicted_tokens, original_word):\n",
        "    original_sentence_with_original_word = original_sentence.replace('[MASK]', original_word)\n",
        "    original_vector = sentence_model.encode(original_sentence_with_original_word)\n",
        "\n",
        "    sentences_with_predictions = [\n",
        "        original_sentence.replace('[MASK]', token) for token in predicted_tokens\n",
        "    ]\n",
        "    prediction_vectors = sentence_model.encode(sentences_with_predictions)\n",
        "\n",
        "    similarities = cosine_similarity(prediction_vectors, [original_vector])\n",
        "\n",
        "    ranked_suggestions = sorted(\n",
        "        zip(predicted_tokens, similarities[:, 0]),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    return ranked_suggestions\n",
        "\n",
        "\n",
        "model_path = \"./bert-synonyms\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./bert-synonyms\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "sentence_model = SentenceTransformer(\"sentence-transformers/LaBSE\")"
      ],
      "metadata": {
        "id": "Din1FnlWzQQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_dict"
      ],
      "metadata": {
        "id": "BSMdGQ681xY4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Была ў Фёдара яшчэ адна рэдкая [MASK]: па сваёй уласнай ініцыятыве ён ніколі нікога не падвозіў, не падбіраў.\"\n",
        "original_word = \"асаблівасць\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkkLQLOWEUMj",
        "outputId": "2e054157-cac7-41cf-d594-78048f4694c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "асаблівасць: 1.0000\n",
            "адметнасць: 0.9986\n",
            "характарыстыка: 0.9974\n",
            "уласцівасць: 0.9972\n",
            "своеасаблівасць: 0.9961\n",
            "характар: 0.9927\n",
            "спецыфіка: 0.9919\n",
            "атрыбут: 0.9912\n",
            "прыкмета: 0.9900\n",
            "рыса: 0.9897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Жанчына была ўпэўненая ў тым, хто віноўнік гэтай [MASK].\"\n",
        "original_word = \"бяды\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2xVDVcdETY7",
        "outputId": "006d8c6d-aa2a-46d9-dc86-5c1ffc2a0674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "бяды: 1.0000\n",
            "няшчасце: 0.9825\n",
            "гора: 0.9773\n",
            "праблемы: 0.9764\n",
            "нягода: 0.9747\n",
            "нядоля: 0.9712\n",
            "бяздолле: 0.9659\n",
            "зло: 0.9618\n",
            "справы: 0.9615\n",
            "справай: 0.9599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Але ўсё гэта можа адбыцца толькі тады, калі нашы [MASK] збудуцца.\"\n",
        "original_word = \"прадбачанні\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXP4_pLFg9w",
        "outputId": "8f0357c9-00f0-4f65-da28-78a576c13149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "прадгаданне: 0.9848\n",
            "прадказанне: 0.9828\n",
            "прароцтва: 0.9797\n",
            "прагноз: 0.9695\n",
            "пажаданні: 0.9641\n",
            "чаканні: 0.9625\n",
            "просьбы: 0.9506\n",
            "жаданні: 0.9473\n",
            "мары: 0.9471\n",
            "прапановы: 0.9435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Пытанні, прыспешваючы, падганяючы адно другое, адгучалі, і ў пакоі надоўга ўсталявалася [MASK].\"\n",
        "original_word = \"цішыня\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOupinRBGzRj",
        "outputId": "67e206fc-0ad8-418a-c980-a26072d48a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ціша: 0.9924\n",
            "ціш: 0.9812\n",
            "супакой: 0.9797\n",
            "спакой: 0.9782\n",
            "спакойлівасць: 0.9759\n",
            "бязгучнасць: 0.9696\n",
            "галасаванне: 0.9628\n",
            "хваляванне: 0.9607\n",
            "мяжа: 0.9547\n",
            "пуста: 0.9530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"[MASK] у сваім новым рамане «Ноч», які пабачыць свет на пачатку восені, прапаноўвае сваю версію таго, што зробіцца з нашай краінай, калі такое адбудзецца на самай справе.\"\n",
        "original_word = \"Пісьменнік\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm9ItkWFHlrG",
        "outputId": "8a9b2ec0-6375-4877-c1fd-b2bd37e2e240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "пісьменнік: 0.9950\n",
            "літаратар: 0.9945\n",
            "аўтар: 0.9916\n",
            "паэт: 0.9888\n",
            "стваральнік: 0.9841\n",
            "мастак слова: 0.9798\n",
            "журналіст: 0.9765\n",
            "чытач: 0.9750\n",
            "госць: 0.9722\n",
            "сюжэт: 0.9719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Хочаце самастойна смела адкрываць візы, хутка шукаць [MASK] авіяквіткі, атрымліваць навіны пра распродажы авіякампаніяў?\"\n",
        "original_word = \"танныя\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "0pzWhOSFebiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e678514-358b-46b2-e1ab-b52fa106ea62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "танныя: 1.0000\n",
            "дарагія: 0.9960\n",
            "недарагі: 0.9932\n",
            "даступны: 0.9890\n",
            "простыя: 0.9867\n",
            "сабе: 0.9860\n",
            "гандлёвыя: 0.9859\n",
            "бясплатныя: 0.9859\n",
            "даступныя: 0.9852\n",
            "рэдкія: 0.9849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Ён гатовы [MASK] у гэтым ключы любыя праблемы, што ўзніклі.\"\n",
        "original_word = \"абмеркаваць\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJLtTJ5ptnjS",
        "outputId": "309d5d78-04d4-4323-eed1-45e72ca13784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "абмеркаваць: 1.0000\n",
            "абмяркоўваць: 0.9934\n",
            "абгаварыць: 0.9904\n",
            "абдумаць: 0.9581\n",
            "разглядаць: 0.9487\n",
            "разгледзець: 0.9484\n",
            "вырашаць: 0.9463\n",
            "выказваць: 0.9443\n",
            "выказаць: 0.9406\n",
            "вырашыць: 0.9403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Ён [MASK] пакуты, ён супраціўляўся сьмерці.\"\n",
        "original_word = \"цярпеў\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpNt6KMRvGDg",
        "outputId": "29d56334-96dd-479f-f420-9f6ca4926b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "пацярпеў: 0.9960\n",
            "зазнаваць: 0.9867\n",
            "праходзіў: 0.9827\n",
            "пражыў: 0.9827\n",
            "трымаў: 0.9806\n",
            "атрымаў: 0.9806\n",
            "правёў: 0.9802\n",
            "прайшоў: 0.9797\n",
            "атрымліваў: 0.9777\n",
            "насіў: 0.9761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"У Менску [MASK] адкрылі памятны знак у гонар братоў Луцкевічаў.\"\n",
        "original_word = \"ўрачыста\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIdQqeqdqWMV",
        "outputId": "9d921afc-dc9c-4686-a75c-123f10365efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ўрачыста: 1.0000\n",
            "урачыста: 0.9949\n",
            "святочна: 0.9884\n",
            "па-святочнаму: 0.9866\n",
            "велічна: 0.9815\n",
            "масава: 0.9814\n",
            "годна: 0.9794\n",
            "шырока: 0.9792\n",
            "афіцыйна: 0.9770\n",
            "спас: 0.9701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Юнакі і дзяўчаты не павінны [MASK] ставіцца да таго, што адбываецца ў краіне.\"\n",
        "original_word = \"абыякава\"\n",
        "\n",
        "predicted_tokens = get_masked_predictions(sentence, original_word, synonym_dict=synonym_dict)\n",
        "ranked_suggestions = rank_by_similarity(sentence, predicted_tokens, original_word)\n",
        "\n",
        "for word, similarity in ranked_suggestions[:10]:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s22vNv--rGWy",
        "outputId": "39481711-f176-42f5-9c36-33160bfbc299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "абыякава: 1.0000\n",
            "безудзельна: 0.9898\n",
            "безуважна: 0.9886\n",
            "незацікаўлена: 0.9866\n",
            "бестурботна: 0.9864\n",
            "бесклапотна: 0.9854\n",
            "бесцікаўна: 0.9833\n",
            "адчужана: 0.9823\n",
            "няўцямна: 0.9800\n",
            "бясстрасна: 0.9799\n"
          ]
        }
      ]
    }
  ]
}